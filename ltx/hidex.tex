\documentclass{IEEEtran}
\title{Spatiotemporal System Identification - the HIDEX($p,q$) model}
\author{Michael Dewar, Kenneth Scerri and Visakan Kadirkamanathan}

\usepackage{color}
\usepackage{amssymb,amsmath} %the AMS stuff
\usepackage{caption} 
\usepackage[font=footnotesize]{subfig} % These are IEEE specific options. subfig overrides the caption formatting
\usepackage{graphicx} % to include graphics
\usepackage{booktabs} % tables
\usepackage{cite}
\usepackage{algorithm,algorithmic}

\newcommand{\todo}[1]{\textsf{\emph{\textbf{\textcolor{blue}{#1}}}}}
\newcommand{\GP}[2]{\mathrm{GP}(#1,#2)}
\newcommand{\N}[2]{\mathrm{N}(#1,#2)}
\newcommand{\inner}[3]{\langle#1,#2\rangle_{#3}}
\newcommand{\dist}[2]{\|#1\|_{#2}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\trb}{{\bf tr}}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\F}{\mathbb{F}}
\DeclareMathOperator{\G}{\mathbb{G}}
\DeclareMathOperator{\p}{p}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\vecf}{vec}
\DeclareMathOperator{\kron}{\otimes}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\xvec}{\mathbf{x}}
\DeclareMathOperator{\uvec}{\mathbf{u}}
\DeclareMathOperator{\avec}{\mathbf{a}}
\DeclareMathOperator{\bvec}{\mathbf{b}}
\DeclareMathOperator{\dvec}{\mathbf{d}}
\DeclareMathOperator{\fvec}{\mathbf{f}}
\DeclareMathOperator{\gvec}{\mathbf{g}}
\DeclareMathOperator{\yvec}{\mathbf{y}}
\DeclareMathOperator{\psivec}{\boldsymbol{\psi}}
\DeclareMathOperator{\phivec}{\boldsymbol{\phi}}
\DeclareMathOperator{\gammavec}{\boldsymbol{\gamma}}
\DeclareMathOperator{\upsilonvec}{\boldsymbol{\upsilon}}
\DeclareMathOperator{\thetaML}{\theta_{\mathrm{ML}}}
\DeclareMathOperator{\onto}{\rightarrow}
\DeclareMathOperator{\block}{\mathrm{block}}
% Envrionments
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary} 
\newtheorem{remark}{Remark} 
\newtheorem{theorem}{\textbf{Theorem}}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}

\begin{document}
\maketitle

\section{Introduction}

\section{The HIDEX($p,q$) Model}
\label{sec:model}

We first introduce the HIDEX($p,q$) model. We then derive the log-likelihood function and introduce a set of decompositions which will be used in Section \ref{sec:estimation} to make the system identification problem tractable. 

The model is defined across spatial locations $s \in \mathcal{S} = \mathbb{R}^{n_\mathcal{S}}$ and at times $t \in \mathbb{Z}_+$. Here $n_\mathcal{S} \in \{1,2,3\}$ is the spatial dimension of the system under study, and, as time is always one dimensional, it is $n_\mathcal{S}$ that we use to describe the dimension of the model. 

\begin{definition}[Spatial Mixing Kernel]
	Let $\mathcal{S}$ denote an $n_\mathcal{S}$-dimensional physical space and $f_t(s): \mathbb{R}^{n_\mathcal{S}} \onto \mathbb{R}$ denote the spatial field across space $s \in \mathcal{S}$ at time $t \in \mathbb{Z}_+$. Then the kernel $k(s,r): \mathbb{R}^{n_\mathcal{S}} \times \mathbb{R}^{n_\mathcal{S}} \onto \mathbb{R}$ of the integral operator
	\begin{equation}
		(\mathbb{K}f_t)(s) = \int k(s,r) f_{t-1}(r) dr
	\end{equation}
	for $r \in \mathcal{S}$ is known as a \emph{spatial mixing kernel}. If $k(s,r) = k(s-r) ~ \forall r$ the spatial mixing kernel is known as a \emph{spatially homogenous spatial mixing kernel}.
\end{definition}

\begin{definition}[HIDEX Model]
	\label{def:hidex}
	The linear spatiotemporal integrodifference equation with exogenous inputs (HIDEX($p,q$)) model is defined by the following equations
	\begin{equation}
		\label{eqn:hidex-hidden}
	f_t(s)= \sum_{i=1}^{p} (\F_i f_{t-i})(s) + \sum_{j=1}^{q}(\G_j g_{t-j})(s)+e(s)
	\end{equation}
	\begin{equation}
	\label{eqn:hidex-obs}
	y_t(s_k) = (\mathbb{H}f_t)(s_k) + \epsilon
	\end{equation}
	where $\F_i,\G_j,\mathbb{H}$ are operators with spatial mixing kernels $F_i(s,r), G_j(s,r), H(s,r)$ respectively. Here $f_t(s)$ is the hidden field and $g_{t}(s)$ is the input field at time $t$; $e(s)$ is the disturbance. The nonnegative model orders $p,q$ are the orders of the autoregression and the input respectively. The output $y_t(s_k)$ is a point in the observable field at spatial location $s_k$, corrupted by the noise process $\epsilon$.
\end{definition}

\begin{assumption}[Stochasticity]
	\label{ass:noise}
	The following assumptions are made about the stochasticity of the HIDEX($p,q$) given in Definition \ref{def:hidex}.
	\begin{itemize}
		\item The disturbance $e(s) \sim \mathrm{GP}(0,Q(s,s'))$ is assumed i.i.d, drawn from a Gaussian Process with zero mean and covariance function $Q(s,s')$.
		\item The observation noise $\epsilon \sim N(0,\sigma)$ is assumed i.i.d, drawn from a univariate normal distribution with zero mean and variance $\sigma$.
		\item The initial field $f_0(s) \sim \mathrm{GP}(0,Q(s,s'))$ is drawn from the same distribution as $e(s)$.
	\end{itemize}
\end{assumption}

\begin{assumption}[Input and Output]
	\label{ass:inout}
The following assumptions are made on the input and output of the system under study:
	\begin{itemize}
		\item Input fields $g_t(s)$ are known exactly for all $s$ and $t$.
		\item Collected observations $Y = \{ y_t(s) : t = 1 \ldots T, ~ s \in \{s_1, s_2 \ldots s_m\} \}$ are generated by an HIDEX process of Definition \ref{def:hidex}.
	\end{itemize}
\end{assumption}

\subsection{Deriving the Log-Likelihood}

Let the unknown parameter set be defined as $\theta = \{F_1, F_2, \ldots F_p, G_1,G_2,\ldots G_q, H\}$ - the set of all the spatial mixing kernels that together define the HIDEX model. Let $\mathcal{F} = \{f_t(s) : t = 1 \ldots T\}$ denote the set of fields associated with each time point of the collected observations and similarly $\mathcal{G} = \{g_t(s) : t = 1 \ldots T\}$ denotes all the inputs. The likelihood factorises along the independence structures defined by the model formulation:
\begin{equation}
	\begin{split}
	p(\mathcal{F}, Y;\theta) =  p(f_0) \prod_{t=1}^T p(\yvec_t|f_t)
	\prod_{t=1}^T  p(f_t|\mathcal{F}_{t-1}^{t-p}, \mathcal{G}_{t-1}^{t-q}) 
	\end{split}
\end{equation}
where $\yvec_t = [y_t(s_1) ~ y_t(s_2) ~ \ldots ~ y_t(s_m)]^\top$, the vector formed by evaluating $y_t(s)$ of Equation \ref{eqn:hidex-obs} at each of the $m$ observation locations. Here $\mathcal{F}_{t-1}^{t-p} = \{f_{t-1} \ldots f_{t-p}\}$ and similarly $\mathcal{G}_{t-1}^{t-q}= \{g_{t-1} \ldots g_{t-q}\}$. Note that we have dropped the dependence of the distributions on $\theta$, and the spatial argument of the fields $f_t$ and $g_t$ for notational convenience. Using Assumption \ref{ass:noise}, these distributions are
\begin{equation}
	\label{eqn:ydist}
	p(\yvec_t|f_t) = \N{\hat{\yvec}_t}{R}
\end{equation}
\begin{equation}
	\label{eqn:fdist}
	p(f_t|\mathcal{F}_{t-1}^{t-p}, \mathcal{G}_{t-1}^{t-q}) = \GP{ \hat{f}_t}{Q(s,s')}
\end{equation}
where $R = \sigma I_m$,
\begin{equation}
	\hat{f}_t =\sum_{i=1}^{p}( \F_i f_{t-i}) + \sum_{j=1}^{q}(\G_j g_{t-j})
\end{equation}
and 
\begin{equation}
	\hat{\mathbf{y}}_t = (\mathbb{H}f_t)([s_1 \ldots s_m]^\top)
\end{equation}
which denotes the vector formed by the evaluation of $(\mathbb{H}f_t)$ at all the observation locations. Here $I_m$ denotes the $m\times m$ identity matrix.

If we define the log likelihood as
\begin{equation}
	\label{eqn:loglik}
	\ell(\theta) = \ln p(\mathcal{F}, Y;\theta)
\end{equation}
then, using the notation
\begin{equation}
	\inner{f}{g}{Q} = \int_\mathcal{S} \int_\mathcal{S} ds~ds' f(s') Q(s',s) g(s)
\end{equation}
\begin{equation}
	\dist{f}{Q} = \inner{f}{f}{Q}
\end{equation}
for functions and similarly
\begin{equation}
	\dist{y}{R} = y^\top Ry
\end{equation}
for vectors, we can substitute Equations \ref{eqn:ydist} and \ref{eqn:fdist} into the log likelihood \ref{eqn:loglik} to get:
\begin{equation}
	\ell(\theta) =  \epsilon  - \frac{1}{2}\sum_t\dist{f_t - \hat{f}_t}{Q^{-1}} - \frac{1}{2}\sum_t\dist{\yvec_t-\hat{\yvec_t}}{R^{-1}}
\end{equation}
where $\epsilon$ collects together all the terms that do not depend on (any element of) $\theta$.

\subsection{Decomposition}

\todo{Don't know how to word these assumptions, but quite like the fact that they exist. Maybe VK's thesis does this sort of thing and the wording is correct in there?}

\begin{assumption}[Field Decompositions]
	\label{ass:fielddecomp}
	We assume the following decompositions of the hidden and input fields can be made exactly:
\begin{equation}
	\label{eq:f decomp}
	f_t(s) = \xvec_t^\top\phi(s)
\end{equation}
\begin{equation}
	g_t(s) = \phi(s)^\top\uvec_t
\end{equation}	 
where $\xvec_t \in \R^{n_x}$ is a vector of unknown weights, referred to as the state of the hidden field at time $t$, and $\uvec_t \in \R^{n_x}$ is a vector of known weights, referred to as the input at time $t$. Here, $\phi(s) : \mathbb{R}^{n_\mathcal{S}} \onto \mathbb{R}$ is a vector of basis functions drawn from the spanning set thingy.
\end{assumption}

\begin{assumption}[Kernel Decompositions]
	\label{ass:kerndecomp}
	We assume the following decompositions of the HIDEX model kernels can be made exactly:
	\begin{equation}
	F_i(s,s') = a_i^\top\psi(s,s')
\end{equation}
\begin{equation}
	G_j(s,s') = b_j^\top\psi(s,s')
\end{equation}
\begin{equation}
	H(s,s') = c^\top\psi(s,s')
\end{equation}
where $a_i, b_j, c \in \R^{n_y}$ are the parameters of the kernels $F_i, G_j$ and $H$ respectively. Here $\psi(s,s') : \mathbb{R}^{n_\mathcal{S}} \times \mathbb{R}^{n_\mathcal{S}} \onto \mathbb{R}$ is a vector of basis functions drawn from the spanning set thingy.
\end{assumption}

\begin{remark}
	The above assumptions will not hold if $\mathcal{S}$ is defined as a subset of $\mathbb{R}^{n_\mathcal{S}}$. This is due to the fact that here the field $f$ would no longer be band limited due to the discontinuity at the edges of $\mathcal{S}$. It is worth noting that while we represent the field over an infinite space, the representation will be largely uninformative outside the observed region of interest. If we use local basis functions, such as squared exponential bases, then the representation of the field away from the centres of the bases will be described almost completely by the properties of the disturbance process.
\end{remark}

\begin{remark}
As a notational convenience, we have used the same basis functions in the decomposition of both the hidden field $f$ and the input field $g$. However, in practice, we may have a control surface that is local in the space $\mathcal{S}$, and hence would benefit from a specific, tailored decomposition. 
\end{remark}

\begin{remark}
	It is useful for what follows to derive the distribution of the system states from the distribution of the field. By substituting the decomposition \ref{eq:f decomp} into the field distribution \ref{eqn:fdist}, we get
	\begin{equation}
		p(x_t\phi(s)) = \mathrm{GP}(\hat{x}^\top\phi(s),Q(s,s))
	\end{equation}
	We can expand the left hand side trivially and, noting that we treat $\phi$ as a deterministic function:
	 \begin{equation}
		p(x_t|\phi(s))p(\phi(s)) = \mathrm{N}(\hat{x},\Sigma_w)\delta(\phi(s))
	\end{equation}
	where $\delta$ is the Dirac measure and we use the property that any finite sample from a Gaussian Process is normally distributed. Here the covariance matrix is given by
	\begin{equation}
		\Sigma_w = \Phi^{-1}\dist{\phi}{Q} \Phi^{-1}
	\end{equation}
	which follows directly by substituting $f_t(s) = x_t^\top\phi(s)$ into the definition of the covariance function $Q(s,s')$ \cite{Scerri09}.
\end{remark}

\section{Parameter Estimation}
\label{sec:estimation}

In order to estimate the shape of the spatial mixing kernels, we apply the EM algorithm, following the derivation given in \cite{GibsonNinness}. We first derive the E-step, giving an expression for the distribution of the field at each point in time, given the data. We then derive the M-step, giving an analytical maximum of the lower bound on the likelihood.

The key insights here are:
\begin{itemize}
	\item we can represent the expected log likelihood function for the HIDEX($p,q$) model as a quadratic in the weights of the spatial mixing kernel decompositions,
	\item generating distributions over the hidden field becomes a straightforward application of the Rauch-Tung-Streibel smoother \cite{Rauch} under the field decompositions.
\end{itemize} 

\subsection{EM Algorithm}

The EM algorithm is a well known method for jointly estimating the state sequence and parameters of state space models. It is by no means the only method: subspace \ref{subspace review}, variational-Bayes \ref{Hinton} and sampling \ref{Doucet} approaches are also popular in this context. Our aim here is to demonstrate that the model presented above can be estimated using \emph{standard} system identification techniques. It should be noted, however, that the EM algorithm lends itself naturally to the case where the system matrices are parameterised, and hence presents a good starting point for the identification of the HIDEX($p,q$) model.

The EM algorithm in this context seeks to find the maximum likelihood parameter estimates 
\begin{equation}
\thetaML=\arg\max_{\theta} \p (Y ; \theta)
\end{equation}
in the case where $\p(Y;\theta)$ is difficult to maximise directly due to a dependence on a hidden variable $X$. By exploiting the relationship between $\p(Y; \theta)$ and $\p(X,Y; \theta)$, it is possible to generate a sequence of parameter estimates that converges on the maximum likelihood parameter estimate.

The EM algorithm achieves this by maximising a lower bound on the log likelihood
\begin{equation}
	\mathcal{Q}(\theta,\theta')=\E_{\theta'}\left[\ln p(X,Y;\theta)\right]
\end{equation}
where $\E_{\theta'}[\cdot]$ denotes expectation taken with respect to the distribution $\p(X|Y ; \theta')$ where $\theta'$ is the current parameter estimate. In this context \cite{Gibson05}, the algorithm consists of two steps: the E-step performs the expectation using the standard Rauch-Tung-Streibel (RTS) smoother \cite{Rauch65} producing the expected state sequence and therefore leading to a deterministic $\mathcal{Q}$-function which is a quadratic in $\theta$. The M-step finds the updated parameter estimate by maximising this quadratic.

\subsection{E-step}

The E-step involves forming the expectation in $Q(\theta,\theta')$, which itself involves finding the distribution over the field $f_t$ given all the data $Y$ and the current kernel estimates $\theta'$. We demonstrate that, using the field decompositions in Assumption \ref{ass:fielddecomp}, this distribution is provided by the RTS smoother. 

Under the decomposition (\ref{eq:f decomp}) we can rewrite 
\begin{equation}
	\dist{f_t(s) - \hat{f}_t(s)}{Q^{-1}} = \dist{\xvec_t - \hat{\xvec}_t}{\Xi}
\end{equation}
where $\Xi = \int\int ds' ds ~ \phi(s')Q^{-1}(s',s)\phi(s)^\top$. Here the vector $\xvec_t \in \R^{n_x}$ weights a vector of basis functions $\phi(s)$ that together decompose the current field $f_t(s)$, and similarly $\hat{\xvec}_t \in \R^{n_x}$ weights the decomposition of $\hat{f}(s)$. This allows us to replace the infinite dimensional term in the likelihood with a finite dimensional term via the decomposition, giving
\begin{equation}
	\ell(\theta) =  \epsilon  - \frac{1}{2}\dist{\xvec_t - \hat{\xvec}_t}{\Xi} - \frac{1}{2}\dist{\yvec_t-\hat{\yvec_t}}{R^{-1}}
\end{equation}
In order to derive an expression for $\hat{x}_t$, recall that
\begin{equation}
	\hat{f}_t(s) = \sum_{i=1}^{p}( \F_i f_{t-i})(s) + \sum_{j=1}^{q}(\G_j g_{t-j})(s)
\end{equation}
Using the decomposition of the fields $f_t(s)$ and $g_t(s)$ given in Assumption \ref{ass:fielddecomp} this can be re-written
\begin{equation}
	\phi(s)^\top\hat{\xvec}_t = \sum_{i=1}^{p}(\F_i \phi^\top)(s)\xvec_{t-i} + \sum_{j=1}^{q}(\G_j\phi^\top)(s)\uvec_{t-j}
\end{equation}
To isolate $\hat{\xvec}_t$ on the left hand side, we pre-multiply both sides by $\phi(s)$, integrate over space, and pre-multiply both sides by the resulting matrix, giving
\begin{equation}
	\begin{split}
	\hat{\xvec}_t = 
		\sum_{i=1}^{p} \Phi^{-1} \int_\mathcal{S}ds \phi(s)(\F_i \phi^\top)(s)\xvec_{t-i} + \\
	\sum_{j=1}^{q} \Phi^{-1} \int_\mathcal{S}ds \phi(s)(\G_j\phi^\top)(s)\uvec_{t-j}
	\end{split}
\end{equation}
where $\Phi = \int_\mathcal{S}ds\phi(s)\phi^\top(s)$.

\begin{remark}
	By defining the following matrices
	\begin{equation}
		A_i = \Phi^{-1} \inner{\phi(s)}{\phi^\top(s)}{F_i}
	\end{equation}
	\begin{equation}
		B_j = \Phi^{-1} \inner{\phi(s)}{\phi^\top(s)}{G_j}
	\end{equation}
	we can write an expression for $\hat{x}$ in multivariate ARX($p,q$) \cite{Ljung} form:
	\begin{equation}
		\label{eqn:arx}
		\hat{\xvec}_t = \sum_{i=1}^{p} A_i^\top \xvec_{t-i} +  \sum_{j=1}^{q} B_j^\top \uvec_{t-j}
	\end{equation}
	making clear the effect of the decomposition on the HIDEX model in Definition \ref{def:hidex}: we are representing an infinite dimensional spatiotemporal model using a finite (although probably high) dimensional vector autoregressive model with exogenous inputs, via a basis function decomposition of the field. The key to this approach is that, unlike standard vector autoregressive models, the number of field basis functions $n_x$, is independent to the complexity of $F_i$ or $G_j$. Therefore we can increase the spatial resolution, or spatial extent, of our model without generating more parameters to learn.
\end{remark}

\todo{show how this can be placed into the standard state-space formulation, and hence applicable for filtering using the RTS smoother}

This recursive relationship between the field weights over time, and between the field weights and the observations, defines a linear, Gaussian state space model. Hence the posterior distribution of the states $x_t$ given a parameter set $\theta'$, and therefore the posterior distribution of the set of fields $\mathcal{F}$, is given by the very well-known Rauch-Tung-Streibel smoother (sometimes called the Kalman Smoother).

\subsection{M-step}

We need to maximise the expected log-likelihood function
\begin{equation}
	\label{eqn:exploglik}
	\begin{split}
	Q(\theta,\theta')] = \epsilon - \frac{1}{2}\sum_t\E_{\theta'}[\dist{f_t(s) - \hat{f}_t(s)}{Q^{-1}}]  \\
	- \frac{1}{2}\sum_t\E_{\theta'}[\dist{\yvec_t-\hat{\yvec}_t}{R^{-1}}]
	\end{split}
\end{equation}
with respect to the unknown parameters $\theta$. Noting that 
\begin{eqnarray}
	\lefteqn{\E\left[\int_\mathcal{S}\int_\mathcal{S}ds ds' f_t(s) Q^{-1}(s,s') \hat{f}_t(s')\right] } \nonumber \\ 
		&& = \int_\mathcal{S}\int_\mathcal{S}ds ds' \E[f_t(s)] Q^{-1}(s,s') \hat{f}_t(s') \\
		&& = \inner{\E[f_t(s)]}{\hat{f}(s)}{Q^{-1}}
\end{eqnarray}
due to the fact that the expectation of the mean field $\hat{f}$ is itself, we can simplify this expression by expanding each term, to give
\begin{equation}
	\begin{split}
	\label{eqn:exloglik}
	Q(\theta,\theta')] = \epsilon
	- \inner{\E_{\theta'}[f_t(s)]}{\hat{f}_t(s)}{Q^{-1}}
	- \frac{1}{2}\sum_t \dist{\hat{f}_t(s)}{Q^{-1}} \\
	- \inner{\E_{\theta'}[\yvec_t]}{\hat{\yvec}_t}{R^{-1}}
	- \frac{1}{2}\sum_t \dist{\hat{\yvec}_t}{R^{-1}}
	\end{split}
\end{equation}
where $\epsilon$ has been abused somewhat to include additional terms that do not depend on the unknown parameters $\theta$. We will deal with these two sets of terms in turn, using the decompositions to write them explicitly in terms of the unknown parameters.

Firstly, we can substitute for $\hat{f}$, rewriting the first two terms in Equation \ref{eqn:exloglik} as
\begin{equation}
	\label{eqn:Eff}
	\inner{\E[f_t(s)]}{\hat{f}_t}{Q^{-1}} = \inner{\E[f_t(s)]}{\sum_i(\mathbb{F}_i\hat{f}_{t-i}) + \sum_j(\mathbb{G}g_{t-j})}{Q^{-1}}
\end{equation}
\begin{equation}
	\label{eqn:fhat}
	\dist{\hat{f}_t}{Q^{-1}} = \dist{\sum_i(\mathbb{F}_i\hat{f}_{t-i}) + \sum_j(\mathbb{G}g_{t-j})}{Q^{-1}}
\end{equation}
where we have dropped the spatial argument on the integral operator for notational convienince (hence $(\mathbb{F}_i\hat{f}_{t-i})(s)$ is written simply $(\mathbb{F}_i\hat{f}_{t-i})$). 

Equation \ref{eqn:Eff} can be expanded to give
\begin{equation}
	\begin{split}
	\inner{\E[f_t(s)]}{\hat{f}_t}{Q^{-1}} 
	 = \inner
		{\E[f_t(s)]}
		{\sum_i(\mathbb{F}_i\hat{f}_{t-i})}
		{Q^{-1}} \\
	+ \inner
		{\E[f_t(s)]}
		{\sum_j(\mathbb{G}_jg_{t-j})}
		{Q^{-1}}
	\end{split}
\end{equation}
Similarly, \ref{eqn:fhat} can be expanded to give
\begin{equation}
	\begin{split}
	\dist
		{\hat{f}_t}
		{Q^{-1}} 
	= & \dist
		{\sum_i(\mathbb{F}_i\hat{f}_{t-i})}
		{Q^{-1}} 
	+ \dist
		{\sum_j(\mathbb{G}g_{t-j})}
		{Q^{-1}}\\
	& + \inner
		{\sum_i(\mathbb{F}_i\hat{f}_{t-i})}
		{\sum_j(\mathbb{G}g_{t-j})}
		{Q^{-1}}
	\end{split}
\end{equation}
To further simplify the expected log-likelihood, note that
\begin{equation}
	\dist{\sum_i(\mathbb{F}_i\hat{f}_{t-i})}
	{Q^{-1}} = 
	\sum_i \sum_{i'} \inner
		{(\mathbb{F}_i\hat{f}_{t-i})}
		{(\mathbb{F}_{i'}\hat{f}_{t-i'})}
		{Q^{-1}}
\end{equation}
and, under the decomposition given in Assumption \ref{ass:kerndecomp}, we can write
\begin{equation}
	\inner
		{(\mathbb{F}_i\hat{f}_{t-i})}
		{(\mathbb{F}_{i'}\hat{f}_{t-i'})}
		{Q^{-1}} =  
	 a_i^\top \inner 
		{(\mathbb{P} f_{t-i'})}
		{(\mathbb{P}^\top f_{t-i'}) }
		{Q^{-1}} a_i
\end{equation}
where we define a new operator $(\mathbb{P} f)(s) = \int_\mathcal{S}dr~\psi(s,r)f_t(r)$. Hence it is straightforward to write the first term of \ref{eqn:exploglik} as
\begin{equation}
	\label{eqn:norm_x_error}
	\begin{split}
	\frac{1}{2}\sum_t\E[\dist{f_t(s) - \hat{f}_t(s)}{Q^{-1}}] = 
		\sum_i a_i^\top \gamma^a_i + \sum_j b_j^\top \gamma^b_j \\
		+ \sum_i\sum_{i'}a_i^\top \Gamma_{i,i'}^a
		a_{i'} 
		+ \sum_j\sum_{j'}b_j^\top \Gamma_{i,j'}^b
		b_{j'} \\
		+ \sum_i\sum_{j}a_i^\top \Gamma_{i,j}^{ab}
		b_j
		\end{split}
\end{equation}
\todo{I've forgotten the $\sum_t$ somewhere but I think they can be moved into the definitions of the various lambdas and Gammas below...}
where
\begin{equation}
	\gamma_i^a = \inner
	{\E[f_t(s)]}
	{(\mathbb{P} f_{t-i})}
	{Q^{-1}}
\end{equation}
\begin{equation}
	\gamma_j^b = \inner
	{\E[f_t(s)]}
	{(\mathbb{P} g_{t-j})}
	{Q^{-1}}
\end{equation}
\begin{equation}
	 \Gamma_{i,i'}^a = \inner
		{(\mathbb{P} f_{t-i})}
		{(\mathbb{P}^\top f_{t-i'})}
		{Q^{-1}}
\end{equation}
\begin{equation}
	\Gamma_{j,j'}^b = \inner
		{(\mathbb{P} g_{t-j})}
		{(\mathbb{P}^\top g_{t-j'})}
		{Q^{-1}}
\end{equation}
\begin{equation}
	\Gamma_{i,j}^{ab} = \inner
		{(\mathbb{P} f_{t-i})}
		{(\mathbb{P}^\top g_{t-j})}
		{Q^{-1}}
\end{equation}

We can write the second term \ref{eqn:exploglik} as quadratic in the unknonwn parameters $c$ following the same procedure: simply expand $\E[\dist{\yvec_t-\hat{\yvec}_t}{R^{-1}}]$ and apply the kernel decomposition given in Assumption \ref{ass:kerndecomp}. This gives
\begin{equation}
	\label{eqn:norm_y_error}
	\E[\dist{\yvec_t-\hat{\yvec}_t}{R^{-1}}] = c^\top \gamma^c + c^\top \Gamma^c c
\end{equation}
where \todo{The following expressions for $y_t$ need checking!}
\begin{equation}
	\gamma_c = \inner
	{\E[y_t]}
	{(\mathbb{P} f_t)}
	{R^{-1}}
\end{equation}
\begin{equation}
	 \Gamma^c = \inner
		{(\mathbb{P} f_t)}
		{(\mathbb{P}^\top f_t)}
		{R^{-1}}
\end{equation}
Together, \ref{eqn:norm_x_error} and \ref{eqn:norm_y_error} allow us to write the expected log likelihood in the quadratic form:
\begin{equation}
	\E[\ell(\theta)] = \epsilon - \boldsymbol{\theta}^\top \boldsymbol{\gamma} + \boldsymbol{\theta}^\top \boldsymbol{\Gamma} \boldsymbol{\theta}
\end{equation}
where we concatenate the unknown parameters into the vector $\boldsymbol{\theta} = [a_1^\top ~ a_2^\top ~ \ldots ~ a_p^\top ~ b_1^\top ~ b_2^\top ~ \ldots ~ b_q^\top ~ c^\top]^\top$ and similarly the linear coefficient into the vetor vector $\boldsymbol{\gamma} = [\gamma^{a\top}_1 ~ \gamma^{a\top}_2 ~ \ldots ~ \gamma^{a\top}_p ~ \gamma^{b\top}_1 ~ \gamma^{b\top}_2 ~ \ldots ~ \gamma^{b\top}_q ~ \gamma^{c\top}]^\top$. The matrix $\Gamma$ is formed by simply arranging all the $\Gamma$-variables in a large matrix. To see this, let 
\begin{equation}
	[\boldsymbol{\Gamma}^a]_{i,i'} = \Gamma_{i,i'}^a 
\end{equation}
\begin{equation}
	[\boldsymbol{\Gamma}^b]_{j,j'} = \Gamma_{j,j'}^b 
\end{equation}
\begin{equation}
	[\boldsymbol{\Gamma}^{ab}]_{i,j} = \Gamma_{i,j}^{ab}
\end{equation}
for $i,i' = 1 \ldots p$ and $j,j' = 1 \ldots q$ where the notation $[\boldsymbol{\Gamma}]_{i,j}$ denotes the $(i,j)^\mathrm{th}$ block of the matrix $\boldsymbol{\Gamma}$. Given these matrices, we can write $\boldsymbol{\Gamma}$ as 
\begin{equation}
	\boldsymbol{\Gamma} = \begin{bmatrix}
		\boldsymbol{\Gamma}^a & \boldsymbol{\Gamma}^{ab} & 0_{n\times n} \\
		\boldsymbol{\Gamma}^{ab\top} & \boldsymbol{\Gamma}^{b} & 0_{n\times n} \\
		0_{n\times n} & 0_{n\times n} & \boldsymbol{\Gamma}_c
	\end{bmatrix}
\end{equation}

\todo{sort out hats!}

\newpage

\end{document}